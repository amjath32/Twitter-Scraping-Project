
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "\n",
        "# Set the username and text query parameters\n",
        "username = \"#csk\"\n",
        "text_query = \"doctor\"\n",
        "\n",
        "# Set the maximum number of tweets to scrape and the date range for the text query\n",
        "max_results = 800\n",
        "since_date = \"2021-12-01\"\n",
        "until_date = \"2023-01-20\"\n",
        "\n",
        "# Scrape tweets with username\n",
        "os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(max_results, username))\n",
        "\n",
        "# Load the scraped tweets into a pandas dataframe\n",
        "tweets_df1 = pd.read_json('user-tweets.json', lines=True)\n",
        "\n",
        "# Extract the desired fields from the tweets dataframe\n",
        "tweets_df1 = tweets_df1[['id', 'date', 'url', 'renderedContent', 'likeCount', 'retweetCount']]\n",
        "\n",
        "# Rename the columns to match the desired output format\n",
        "tweets_df1 = tweets_df1.rename(columns={'url': 'permalink', 'renderedContent': 'content'})\n",
        "\n",
        "# Add a column for the username\n",
        "tweets_df1['username'] = tweets_df1['permalink'].apply(lambda x: x.split('/')[-3])\n",
        "\n",
        "# Save the extracted tweets to a CSV file\n",
        "tweets_df1.to_csv('user-tweets.csv', sep=',', index=False)\n",
        "\n",
        "# Scrape tweets with text query\n",
        "os.system('snscrape --jsonl --max-results {} --since {} twitter-search \"{} until:{}\"> text-query-tweets.json'.\n",
        "          format(max_results, since_date, text_query, until_date))\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import pymongo as pym\n",
        "\n",
        "# Define the MongoDB connection details\n",
        "client = pym.MongoClient(\"mongodb+srv://@cluster0.49sva5i.mongodb.net/?retryWrites=true&w=majority\")\n",
        "db = client[\"TWITTERSCRAPING\"]\n",
        "collection = db[\"SCRAPED_DATA\"]\n",
        "\n",
        "# Load the user-tweets CSV file into a pandas dataframe\n",
        "user_tweets_df = pd.read_csv(\"/content/user-tweets.csv\")\n",
        "\n",
        "# Convert the pandas dataframe to a list of dictionaries\n",
        "user_tweets_data = user_tweets_df.to_dict(\"records\")\n",
        "\n",
        "# Insert the list of dictionaries into the MongoDB collection\n",
        "collection.insert_many(user_tweets_data)\n",
        "\n",
        "# Load the text-query-tweets CSV file into a pandas dataframe\n",
        "text_query_tweets_df = pd.read_csv(\"/content/text-query-tweets.csv\")\n",
        "\n",
        "# Convert the pandas dataframe to a list of dictionaries\n",
        "text_query_tweets_data = text_query_tweets_df.to_dict(\"records\")\n",
        "\n",
        "# Insert the list of dictionaries into the MongoDB collection\n",
        "collection.insert_many(text_query_tweets_data)\n",
        "\n",
        "\n",
        "# Load the scraped tweets into a pandas dataframe\n",
        "tweets_df2 = pd.read_json('text-query-tweets.json', lines=True)\n",
        "\n",
        "# Extract the desired fields from the tweets dataframe\n",
        "tweets_df2 = tweets_df2[['id', 'date', 'url', 'renderedContent', 'likeCount', 'retweetCount']]\n",
        "\n",
        "# Rename the columns to match the desired output format\n",
        "tweets_df2 = tweets_df2.rename(columns={'url': 'permalink', 'renderedContent': 'content'})\n",
        "\n",
        "# Add a column for the username\n",
        "tweets_df2['username'] = tweets_df2['permalink'].apply(lambda x: x.split('/')[-3])\n",
        "\n",
        "# Save the extracted tweets to a CSV file\n",
        "tweets_df2.to_csv('text-query-tweets.csv', sep=',', index=False)\n"
      ],
      "metadata": {
        "id": "1GKM3bvOn7nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import pymongo\n",
        "\n",
        "# Connect to MongoDB\n",
        "client = pymongo.MongoClient(\"mongodb+srv://@cluster0.49sva5i.mongodb.net/?retryWrites=true&w=majority\")\n",
        "db = client[\"TWITTERSCRAPING\"]\n",
        "collection = db[\"SCRAPED_DATA\"]\n",
        "\n",
        "# Load the data from MongoDB into a pandas dataframe\n",
        "data = pd.DataFrame(list(collection.find()))\n",
        "\n",
        "# Create a sidebar for filtering by username and text query\n",
        "st.sidebar.title(\"Filters\")\n",
        "username_filter = st.sidebar.text_input(\"Username\")\n",
        "text_query_filter = st.sidebar.text_input(\"Text Query\")\n",
        "\n",
        "# Filter the data based on the sidebar inputs\n",
        "if username_filter:\n",
        "    data = data[data['username'].str.lower().str.contains(username_filter.lower())]\n",
        "if text_query_filter:\n",
        "    data = data[data['content'].str.lower().str.contains(text_query_filter.lower())]\n",
        "\n",
        "# Display the data in a table\n",
        "st.title(\"Twitter Data\")\n",
        "st.table(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGb4GK3ZoBa9",
        "outputId": "dcd75a8e-20f2-4bac-b8a9-5215d8b75448"
     
