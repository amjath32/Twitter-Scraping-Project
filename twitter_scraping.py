# -*- coding: utf-8 -*-
"""Twitter Scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UQvYS2NmRD_-Z3BsSbwlefYDCyPSF01P
"""

import os
import json
import pandas as pd
import streamlit as st
import snscrape.modules.twitter as sntwitter

# Set the username and text query parameters
username = "#csk"
text_query = "doctor"

# Set the maximum number of tweets to scrape and the date range for the text query
max_results = 800
since_date = "2021-12-01"
until_date = "2023-01-20"

# Scrape tweets with username
os.system("snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json".format(max_results, username))

# Load the scraped tweets into a pandas dataframe
tweets_df1 = pd.read_json('user-tweets.json', lines=True)

# Extract the desired fields from the tweets dataframe
tweets_df1 = tweets_df1[['id', 'date', 'url', 'renderedContent', 'likeCount', 'retweetCount']]

# Rename the columns to match the desired output format
tweets_df1 = tweets_df1.rename(columns={'url': 'permalink', 'renderedContent': 'content'})

# Add a column for the username
tweets_df1['username'] = tweets_df1['permalink'].apply(lambda x: x.split('/')[-3])

# Save the extracted tweets to a CSV file
tweets_df1.to_csv('user-tweets.csv', sep=',', index=False)

# Scrape tweets with text query
os.system('snscrape --jsonl --max-results {} --since {} twitter-search "{} until:{}"> text-query-tweets.json'.
          format(max_results, since_date, text_query, until_date))


import pandas as pd
import pymongo as pym

# Define the MongoDB connection details
client = pym.MongoClient("mongodb+srv://@cluster0.49sva5i.mongodb.net/?retryWrites=true&w=majority")
db = client["TWITTERSCRAPING"]
collection = db["SCRAPED_DATA"]

# Load the user-tweets CSV file into a pandas dataframe
user_tweets_df = pd.read_csv("C:\Users\amjat\PycharmProjects\twitter scrape 2")

# Convert the pandas dataframe to a list of dictionaries
user_tweets_data = user_tweets_df.to_dict("records")

# Insert the list of dictionaries into the MongoDB collection
collection.insert_many(user_tweets_data)

# Load the text-query-tweets CSV file into a pandas dataframe
text_query_tweets_df = pd.read_csv("/content/text-query-tweets.csv")

# Convert the pandas dataframe to a list of dictionaries
text_query_tweets_data = text_query_tweets_df.to_dict("records")

# Insert the list of dictionaries into the MongoDB collection
collection.insert_many(text_query_tweets_data)


# Load the scraped tweets into a pandas dataframe
tweets_df2 = pd.read_json('text-query-tweets.json', lines=True)

# Extract the desired fields from the tweets dataframe
tweets_df2 = tweets_df2[['id', 'date', 'url', 'renderedContent', 'likeCount', 'retweetCount']]

# Rename the columns to match the desired output format
tweets_df2 = tweets_df2.rename(columns={'url': 'permalink', 'renderedContent': 'content'})

# Add a column for the username
tweets_df2['username'] = tweets_df2['permalink'].apply(lambda x: x.split('/')[-3])

# Save the extracted tweets to a CSV file
tweets_df2.to_csv('text-query-tweets.csv', sep=',', index=False)

import streamlit as st
import pandas as pd
import pymongo

# Connect to MongoDB
client = pymongo.MongoClient("mongodb+srv://@cluster0.49sva5i.mongodb.net/?retryWrites=true&w=majority")
db = client["TWITTERSCRAPING"]
collection = db["SCRAPED_DATA"]

# Load the data from MongoDB into a pandas dataframe
data = pd.DataFrame(list(collection.find()))

# Create a sidebar for filtering by username and text query
st.sidebar.title("Filters")
username_filter = st.sidebar.text_input("Username")
text_query_filter = st.sidebar.text_input("Text Query")

# Filter the data based on the sidebar inputs
if username_filter:
    data = data[data['username'].str.lower().str.contains(username_filter.lower())]
if text_query_filter:
    data = data[data['content'].str.lower().str.contains(text_query_filter.lower())]

# Display the data in a table
st.title("Twitter Data")
st.table(data)
